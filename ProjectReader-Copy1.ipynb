{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1662721a-d4f3-497d-a08d-8bb2c65b4ffb",
   "metadata": {},
   "source": [
    "## Installing the basics required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ab0badf-fc80-43a1-bb86-921e0d2d7907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adarshbhattarai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adarshbhattarai/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adarshbhattarai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "import spacy\n",
    "import nltk\n",
    "import vertexai\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.schema.document import Document\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from googletrans import Translator\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "os.environ['GOOGLE_API_KEY'] = api_key\n",
    "project_id = \"mimetic-fulcrum-407320\"\n",
    "vertexai.init(project=project_id, location=\"us-central1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e227f10e-b12e-4748-9754-10e127faf698",
   "metadata": {},
   "source": [
    "### Preprocess text functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e9816b4-8d26-4d4d-a7cf-747584d3bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "os.environ['GOOGLE_API_KEY'] = api_key\n",
    "project_id = \"mimetic-fulcrum-407320\"\n",
    "vertexai.init(project=project_id, location=\"us-central1\")\n",
    "\n",
    "class Content:\n",
    "    def __init__(self, url, paragraphs):\n",
    "        self.url = url\n",
    "        self.paragraphs = paragraphs\n",
    "\n",
    "with open('allContent.pkl', 'rb') as f:\n",
    "    all_content = pickle.load(f)\n",
    "\n",
    "# Preprocess text functions\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# Initialize knowledge base\n",
    "knowledge_base = {}\n",
    "for content in all_content:\n",
    "    knowledge_base[content.url] = content.paragraphs\n",
    "\n",
    "# Preprocess paragraphs\n",
    "for url, paragraphs in knowledge_base.items():\n",
    "    processed_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        preprocessed_paragraph = preprocess_text(paragraph)\n",
    "        preprocessed_paragraph = remove_stopwords(preprocessed_paragraph)\n",
    "        processed_paragraphs.append(preprocessed_paragraph)\n",
    "    knowledge_base[url] = processed_paragraphs\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Initialize vector store\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [paragraph for paragraphs in knowledge_base.values() for paragraph in paragraphs],\n",
    "    embeddings,\n",
    ")\n",
    "\n",
    "# Initialize retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Initialize conversational memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "all_documents = []\n",
    "\n",
    "for url, paragraphs in knowledge_base.items():\n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph.strip():\n",
    "            document = Document(page_content=paragraph, metadata={'url': url})\n",
    "            all_documents.append(document)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22253389-fec7-45e5-968d-b3b3ededab3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unexpected message type: context. Use one of 'human', 'user', 'ai', 'assistant', or 'system'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 51\u001b[0m\n\u001b[1;32m     23\u001b[0m contextualize_q_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m     24\u001b[0m     [\n\u001b[1;32m     25\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, contextualize_q_system_prompt),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     ]\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m chain_of_thought_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou will act as a professional and knowledgeable dentist. The user, acting as the patient, will provide their details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour role is to diagnose potential dental issues and suggest the best course of action based on their condition. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe response should be clear, concise, and easy to understand. Use paragraphs for general information and bullet points for oral care education.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m )\n\u001b[0;32m---> 51\u001b[0m combined_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mChatPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_of_thought_prompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mMessagesPlaceholder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{input}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{context}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m combined_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m     61\u001b[0m     [\n\u001b[1;32m     62\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, chain_of_thought_prompt),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m     ]\n\u001b[1;32m     66\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_core/prompts/chat.py:978\u001b[0m, in \u001b[0;36mChatPromptTemplate.from_messages\u001b[0;34m(cls, messages, template_format)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_messages\u001b[39m(\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    941\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[1;32m    942\u001b[0m     template_format: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmustache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    943\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatPromptTemplate:\n\u001b[1;32m    944\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[1;32m    945\u001b[0m \n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;124;03m        a chat prompt template\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 978\u001b[0m     _messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    979\u001b[0m         _convert_to_message(message, template_format) \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[1;32m    980\u001b[0m     ]\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;66;03m# Automatically infer input variables from messages\u001b[39;00m\n\u001b[1;32m    983\u001b[0m     input_vars: Set[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_core/prompts/chat.py:979\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_messages\u001b[39m(\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    941\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[1;32m    942\u001b[0m     template_format: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmustache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    943\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatPromptTemplate:\n\u001b[1;32m    944\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[1;32m    945\u001b[0m \n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;124;03m        a chat prompt template\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    978\u001b[0m     _messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 979\u001b[0m         \u001b[43m_convert_to_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[1;32m    980\u001b[0m     ]\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;66;03m# Automatically infer input variables from messages\u001b[39;00m\n\u001b[1;32m    983\u001b[0m     input_vars: Set[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_core/prompts/chat.py:1235\u001b[0m, in \u001b[0;36m_convert_to_message\u001b[0;34m(message, template_format)\u001b[0m\n\u001b[1;32m   1233\u001b[0m message_type_str, template \u001b[38;5;241m=\u001b[39m message\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message_type_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1235\u001b[0m     _message \u001b[38;5;241m=\u001b[39m \u001b[43m_create_template_from_message_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_type_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1239\u001b[0m     _message \u001b[38;5;241m=\u001b[39m message_type_str(\n\u001b[1;32m   1240\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[1;32m   1241\u001b[0m             cast(\u001b[38;5;28mstr\u001b[39m, template), template_format\u001b[38;5;241m=\u001b[39mtemplate_format\n\u001b[1;32m   1242\u001b[0m         )\n\u001b[1;32m   1243\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_core/prompts/chat.py:1193\u001b[0m, in \u001b[0;36m_create_template_from_message_type\u001b[0;34m(message_type, template, template_format)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1187\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected arguments for placeholder message type.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1188\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Expected either a single string variable name\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1189\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or a list of [variable_name: str, is_optional: bool].\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1191\u001b[0m         )\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected message type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Use one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1195\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mai\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1196\u001b[0m     )\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected message type: context. Use one of 'human', 'user', 'ai', 'assistant', or 'system'."
     ]
    }
   ],
   "source": [
    "\n",
    "db = FAISS.from_documents(all_documents, gemini_embeddings)\n",
    "\n",
    "user_prompt_model = \"\"\"Given the user query {query} , present your answer in 3 sentences and make it as clear and concise.\"\"\"\n",
    "\n",
    "rephrase_prompt_model = \"\"\"Given the user query {query} , present your answer in 3 sentences and make it as clear and concise. Be context aware as user has sent history of conversation as well.\"\"\"\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context and latest history if present to answer \"\n",
    "    \"the question. Use three sentences maximum and keep the answer concise. If you don't know the answer, just reply 'NoIdea'.\"\n",
    "    \"\\n\\n{context}\"\n",
    ")\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"use three sentences maximum and keep the answer concise, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain_of_thought_prompt = (\n",
    "    \"You will act as a professional and knowledgeable dentist. The user, acting as the patient, will provide their details. \"\n",
    "    \"Your role is to diagnose potential dental issues and suggest the best course of action based on their condition. \"\n",
    "    \"Use the available knowledge base to provide accurate, context-aware answers, considering the patient's name, age, lifestyle, \"\n",
    "    \"smoker/non-smoker status, medical condition, allergy history, and current context. If the answer is not found in the uploaded content, use Google's LLM. \"\n",
    "    \"Follow these steps:\\n\\n\"\n",
    "    \"1. Summarize the patient's details: age, name, symptoms, medical condition, allergy history, smoker/non-smoker status, and constraints if present.\\n\"\n",
    "    \"2. Ask about their last dental check-up, any previous procedures, or ongoing dental care.\\n\"\n",
    "    \"3. Identify and list possible causes for the patient's symptoms.\\n\"\n",
    "    \"4. Explain the reasoning behind each possible diagnosis.\\n\"\n",
    "    \"5. Use a decision-tree approach to narrow down the most probable cause.\\n\"\n",
    "    \"6. Suggest conventional treatments and provide home remedies and natural alternatives.\\n\"\n",
    "    \"7. Explain why each treatment is recommended.\\n\"\n",
    "    \"8. Specify treatment times and the importance of follow-up appointments.\\n\"\n",
    "    \"9. Consider the patient's age, lifestyle, medical history, and smoker/non-smoker status.\\n\"\n",
    "    \"10. Conduct a risk assessment and recommend preventive measures.\\n\"\n",
    "    \"11. Educate the patient on proper oral care techniques and emphasize the importance of regular checkups.\\n\\n\"\n",
    "    \"The response should be clear, concise, and easy to understand. Use paragraphs for general information and bullet points for oral care education.\"\n",
    ")\n",
    "\n",
    "combined_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", chain_of_thought_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"context\", \"{context}\"), \n",
    "    ]\n",
    ")\n",
    "\n",
    "combined_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", chain_of_thought_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02218d7a-c05e-42b0-94c2-7d97a70e11be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LLM:\n",
    "    def __init__(self):\n",
    "        self.model = VertexAI(model_name=\"gemini-1.0-pro-001\")\n",
    "\n",
    "    def combine_docs_chain(self):\n",
    "        system_prompt = chain_of_thought_prompt  \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ])\n",
    "        return create_stuff_documents_chain(llm=self.model, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f327144-4f21-4544-bc53-6e4934387a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DentalChatbot:\n",
    "    def __init__(self):\n",
    "        self.llm = LLM()\n",
    "        self.model = self.llm.model\n",
    "        self.retriever = db.as_retriever()\n",
    "        self.chat_history = []\n",
    "        self.feedback_counter = 0\n",
    "        self.translator = Translator()\n",
    "\n",
    "    def get_rag_chain(self):\n",
    "        chain = self.llm.combine_docs_chain()\n",
    "        return create_retrieval_chain(self.retriever, chain)\n",
    "\n",
    "    def answer_question(self, patient_details, query, conversation_history=None, lang='en'):\n",
    "        input_data = {\n",
    "            \"patient_details\": patient_details,\n",
    "            \"query\": query\n",
    "        }\n",
    "\n",
    "        if conversation_history:\n",
    "            history_aware_retriever = create_history_aware_retriever(\n",
    "                self.model, self.retriever, contextualize_q_prompt\n",
    "            )\n",
    "            qa_prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", system_prompt),\n",
    "                    MessagesPlaceholder(\"chat_history\"),\n",
    "                    (\"human\", \"{input}\"),\n",
    "                ]\n",
    "            )\n",
    "            question_answer_chain = create_stuff_documents_chain(self.model, qa_prompt)\n",
    "            rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "            result = rag_chain.invoke({\"input\": input_data, \"chat_history\": conversation_history})\n",
    "            answer = result['answer']\n",
    "        else:\n",
    "            chain = self.get_rag_chain()\n",
    "            result = chain.invoke({\"input\": input_data})\n",
    "            answer = result['answer']\n",
    "\n",
    "        if answer == 'NoIdea':\n",
    "            ai_prompt = PromptTemplate.from_template(user_prompt_model)\n",
    "            chain = ai_prompt | self.llm.model\n",
    "            answer = chain.invoke({\"query\": query})\n",
    "\n",
    "        detected_lang = self.translator.detect(query).lang\n",
    "        if detected_lang != 'en':\n",
    "            answer = self.translator.translate(answer, src='en', dest=detected_lang).text\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def interact(self):\n",
    "        while True:\n",
    "            name = input(\"Enter your name: \")\n",
    "            age = input(\"Enter your age: \")\n",
    "            symptoms = input(\"Describe your symptoms: \")\n",
    "            medical_condition = input(\"Do you have any underlying medical conditions? (yes/no): \")\n",
    "            if medical_condition.lower() == \"yes\":\n",
    "                medical_condition = input(\"Please specify your medical conditions: \")\n",
    "            else:\n",
    "                medical_condition = \"None\"\n",
    "            allergy_history = input(\"Do you have any allergies? (yes/no): \")\n",
    "            if allergy_history.lower() == \"yes\":\n",
    "                allergy_history = input(\"Please specify your allergies: \")\n",
    "            else:\n",
    "                allergy_history = \"None\"\n",
    "            smoker_status = input(\"Are you a smoker? (yes/no): \")\n",
    "            current_dental_history = input(\"When was your last dental check-up? Have you had any major procedures? \")\n",
    "\n",
    "            patient_details = {\n",
    "                \"name\": name,\n",
    "                \"age\": age,\n",
    "                \"symptoms\": symptoms,\n",
    "                \"medical_condition\": medical_condition,\n",
    "                \"allergy_history\": allergy_history,\n",
    "                \"smoker_status\": smoker_status,\n",
    "                \"current_dental_history\": current_dental_history\n",
    "            }\n",
    "\n",
    "            question = input(\"Please ask your dental question: \")\n",
    "            if question.lower() == \"quit\":\n",
    "                break\n",
    "            response = self.answer_question(patient_details=patient_details, query=question, conversation_history=self.chat_history)\n",
    "            print(\"Response:\", response)\n",
    "            self.chat_history.extend([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=response),\n",
    "            ])\n",
    "\n",
    "            self.feedback_counter += 1\n",
    "            if self.feedback_counter % 5 == 0:\n",
    "                user_feedback = input(\"Was this answer helpful? (yes/no): \")\n",
    "                if user_feedback.lower() == \"yes\":\n",
    "                    continue\n",
    "                elif user_feedback.lower() == \"no\":\n",
    "                    new_query = input(\"I'm sorry. Can you please provide more details or ask another question: \")\n",
    "                    if new_query:\n",
    "                        response = self.answer_question(patient_details=patient_details, query=new_query, conversation_history=self.chat_history)\n",
    "                        print(\"Response:\", response)\n",
    "                        self.chat_history.extend([\n",
    "                            HumanMessage(content=new_query),\n",
    "                            AIMessage(content=response),\n",
    "                        ])\n",
    "                    else:\n",
    "                        print(\"Okay, let me know if you have any other questions.\")\n",
    "                        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5a98065-bce5-4747-8fef-bb81a4815530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  aasthu\n",
      "Enter your age:  20\n",
      "Describe your symptoms:  pain\n",
      "Do you have any underlying medical conditions? (yes/no):  no\n",
      "Do you have any allergies? (yes/no):  no\n",
      "Are you a smoker? (yes/no):  no\n",
      "When was your last dental check-up? Have you had any major procedures?  2 years back\n",
      "Please ask your dental question:  i saw my gums were bleeding while i was brushing my teeth in the morning. i am having sensitivity since then. how do i treat this at home?\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Prompt must accept context as an input variable. Received prompt with input variables: ['chat_history', 'input']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     chatbot \u001b[38;5;241m=\u001b[39m DentalChatbot()\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mchatbot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 82\u001b[0m, in \u001b[0;36mDentalChatbot.interact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m question\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manswer_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatient_details\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatient_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_history\u001b[38;5;241m.\u001b[39mextend([\n\u001b[1;32m     85\u001b[0m     HumanMessage(content\u001b[38;5;241m=\u001b[39mquestion),\n\u001b[1;32m     86\u001b[0m     AIMessage(content\u001b[38;5;241m=\u001b[39mresponse),\n\u001b[1;32m     87\u001b[0m ])\n",
      "Cell \u001b[0;32mIn[49], line 36\u001b[0m, in \u001b[0;36mDentalChatbot.answer_question\u001b[0;34m(self, patient_details, query, conversation_history, lang)\u001b[0m\n\u001b[1;32m     34\u001b[0m     answer \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     chain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_rag_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     result \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_data})\n\u001b[1;32m     38\u001b[0m     answer \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[49], line 11\u001b[0m, in \u001b[0;36mDentalChatbot.get_rag_chain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_rag_chain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     chain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m create_retrieval_chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever, chain)\n",
      "Cell \u001b[0;32mIn[41], line 12\u001b[0m, in \u001b[0;36mLLM.combine_docs_chain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      6\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m chain_of_thought_prompt  \n\u001b[1;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages([\n\u001b[1;32m      8\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, system_prompt),\n\u001b[1;32m      9\u001b[0m     MessagesPlaceholder(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     10\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     11\u001b[0m ])\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_stuff_documents_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain/chains/combine_documents/stuff.py:74\u001b[0m, in \u001b[0;36mcreate_stuff_documents_chain\u001b[0;34m(llm, prompt, output_parser, document_prompt, document_separator)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_stuff_documents_chain\u001b[39m(\n\u001b[1;32m     23\u001b[0m     llm: LanguageModelLike,\n\u001b[1;32m     24\u001b[0m     prompt: BasePromptTemplate,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     document_separator: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DOCUMENT_SEPARATOR,\n\u001b[1;32m     29\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Runnable[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Any]:\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chain for passing a list of Documents to a model.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m            chain.invoke({\"context\": docs})\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     \u001b[43m_validate_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     _document_prompt \u001b[38;5;241m=\u001b[39m document_prompt \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_DOCUMENT_PROMPT\n\u001b[1;32m     76\u001b[0m     _output_parser \u001b[38;5;241m=\u001b[39m output_parser \u001b[38;5;129;01mor\u001b[39;00m StrOutputParser()\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain/chains/combine_documents/base.py:26\u001b[0m, in \u001b[0;36m_validate_prompt\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_prompt\u001b[39m(prompt: BasePromptTemplate) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DOCUMENTS_KEY \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m prompt\u001b[38;5;241m.\u001b[39minput_variables:\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt must accept \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDOCUMENTS_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as an input variable. Received prompt \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith input variables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;241m.\u001b[39minput_variables\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Prompt must accept context as an input variable. Received prompt with input variables: ['chat_history', 'input']"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = DentalChatbot()\n",
    "    chatbot.interact()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "875a74f9-3b64-4ab2-b95a-f6f5f7ad3540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "few_shot_prompt = \"\"\"\n",
    "You are a knowledgeable and helpful dental assistant. Your task is to answer questions based on the provided context, \n",
    "ensuring that responses are accurate and relevant. Use a generous and formal tone, and always conclude by suggesting\n",
    "that the user visit a professional dentist or book an appointment for the best results. Assist step by step if necessary.\n",
    "\n",
    "User history is like follow:\n",
    "\n",
    "{name} visited dental {dental_visits} and has this medical history: {history}\n",
    "\n",
    "Suggest your answer based on user history. Always start by greeting with {name}\n",
    "\n",
    "{context}\n",
    "\n",
    "Q: {input}\n",
    "A: \n",
    "\"\"\"\n",
    "\n",
    "user_prompt_model = \"\"\"Given the user query {query} , present your answer in 3 sentences and make it as clear and concise.\"\"\"\n",
    "\n",
    "rephrase_prompt_model = \"\"\"Given the user query {query} , present your answer in 3 sentences and make it as clear and concise. Be context aware as user has sent history of conversation as well.\"\"\"\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context and latest history if present to answer \"\n",
    "    \"the question. Use three sentences maximum and keep the answer concise. If you don't know the answer, just reply 'NoIdea'.\"\n",
    "    \"\\n\\n{context}\"\n",
    ")\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"use three sentences maximum and keep the answer concise, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain_of_thought_prompt = (\n",
    "    \"You will act as a professional and knowledgeable dentist. The user, acting as the patient, will provide their details. \"\n",
    "    \"Your role is to diagnose potential dental issues and suggest the best course of action based on their condition. \"\n",
    "    \"Use the available knowledge base to provide accurate, context-aware answers, considering the patient's name, age, lifestyle, \"\n",
    "    \"smoker/non-smoker status, medical condition, allergy history, and current context. If the answer is not found in the uploaded content, use Google's LLM. \"\n",
    "    \"Follow these steps:\\n\\n\"\n",
    "    \"1. Summarize the patient's details: age, name, symptoms, medical condition, allergy history, smoker/non-smoker status, and constraints if present.\\n\"\n",
    "    \"2. Ask about their last dental check-up, any previous procedures, or ongoing dental care.\\n\"\n",
    "    \"3. Identify and list possible causes for the patient's symptoms.\\n\"\n",
    "    \"4. Explain the reasoning behind each possible diagnosis.\\n\"\n",
    "    \"5. Use a decision-tree approach to narrow down the most probable cause.\\n\"\n",
    "    \"6. Suggest conventional treatments and provide home remedies and natural alternatives.\\n\"\n",
    "    \"7. Explain why each treatment is recommended.\\n\"\n",
    "    \"8. Specify treatment times and the importance of follow-up appointments.\\n\"\n",
    "    \"9. Consider the patient's age, lifestyle, medical history, and smoker/non-smoker status.\\n\"\n",
    "    \"10. Conduct a risk assessment and recommend preventive measures.\\n\"\n",
    "    \"11. Educate the patient on proper oral care techniques and emphasize the importance of regular checkups.\\n\\n\"\n",
    "    \"The response should be clear, concise, and easy to understand. Use paragraphs for general information and bullet points for oral care education.\"\n",
    ")\n",
    "\n",
    "combined_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", chain_of_thought_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self):\n",
    "        self.model = VertexAI(model_name=\"gemini-1.0-pro-001\")\n",
    "\n",
    "    def combine_docs_chain(self):\n",
    "        system_prompt = few_shot_prompt  \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ])\n",
    "        return create_stuff_documents_chain(llm=self.model, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a13f0f-d5cd-41bf-9afa-569f23ba0b70",
   "metadata": {},
   "source": [
    "## Dental chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "166b9ada-1eb2-4363-8e0e-ebaef9eceb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  Aastha Poudel\n",
      "Enter your age:  30\n",
      "Describe your symptoms:  teethache\n",
      "Do you have any underlying medical conditions? (yes/no):  yes\n",
      "Please specify your medical conditions:  thyroid\n",
      "Do you have any allergies? (yes/no):  yed\n",
      "Are you a smoker? (yes/no):  no\n",
      "When was your last dental check-up? Have you had any major procedures?  2years\n",
      "Please ask your dental question:  i am having teethache . i alsonoticed blood while i was brushing my teeth. what should i do?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    109\u001b[0m     chatbot \u001b[38;5;241m=\u001b[39m DentalChatbot()\n\u001b[0;32m--> 110\u001b[0m     \u001b[43mchatbot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 82\u001b[0m, in \u001b[0;36mDentalChatbot.interact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m question\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manswer_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatient_details\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatient_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_history\u001b[38;5;241m.\u001b[39mextend([\n\u001b[1;32m     85\u001b[0m     HumanMessage(content\u001b[38;5;241m=\u001b[39mquestion),\n\u001b[1;32m     86\u001b[0m     AIMessage(content\u001b[38;5;241m=\u001b[39mresponse),\n\u001b[1;32m     87\u001b[0m ])\n",
      "Cell \u001b[0;32mIn[28], line 37\u001b[0m, in \u001b[0;36mDentalChatbot.answer_question\u001b[0;34m(self, patient_details, query, conversation_history, lang)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     chain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_rag_chain()\n\u001b[0;32m---> 37\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     answer \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNoIdea\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_core/runnables/base.py:4573\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4567\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4568\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4569\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4570\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4571\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4572\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4574\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4575\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4576\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4577\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_core/runnables/base.py:2502\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2498\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m   2499\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2500\u001b[0m )\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2502\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2504\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_core/runnables/passthrough.py:469\u001b[0m, in \u001b[0;36mRunnableAssign.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    466\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    468\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_core/runnables/base.py:1598\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1594\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1595\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1596\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1597\u001b[0m         Output,\n\u001b[0;32m-> 1598\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1600\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1601\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1602\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1606\u001b[0m     )\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1608\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_core/runnables/config.py:380\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    379\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_core/runnables/passthrough.py:456\u001b[0m, in \u001b[0;36mRunnableAssign._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke\u001b[39m(\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    449\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m\n\u001b[1;32m    452\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    461\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_core/runnables/base.py:3149\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   3136\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3137\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3138\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m   3139\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3147\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3148\u001b[0m         ]\n\u001b[0;32m-> 3149\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_core/runnables/base.py:3149\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3136\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3137\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3138\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m   3139\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3147\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3148\u001b[0m         ]\n\u001b[0;32m-> 3149\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/concurrent/futures/_base.py:440\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/concurrent/futures/_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__get_result\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m--> 389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/concurrent/futures/thread.py:52\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_core/runnables/base.py:4573\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4567\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4568\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4569\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4570\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4571\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4572\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4574\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4575\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4576\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4577\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_core/runnables/base.py:2504\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2502\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2503\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2504\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2505\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_core/retrievers.py:221\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    220\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[1;32m    224\u001b[0m         result,\n\u001b[1;32m    225\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_core/retrievers.py:214\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 214\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_core/vectorstores.py:797\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_relevant_documents\u001b[39m(\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[1;32m    795\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 797\u001b[0m         docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    799\u001b[0m         docs_and_similarities \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    800\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[1;32m    801\u001b[0m                 query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs\n\u001b[1;32m    802\u001b[0m             )\n\u001b[1;32m    803\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_community/vectorstores/faiss.py:530\u001b[0m, in \u001b[0;36mFAISS.similarity_search\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    512\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    517\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \n\u001b[1;32m    520\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_community/vectorstores/faiss.py:402\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search_with_score\u001b[39m(\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    380\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    385\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    386\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m        L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score_by_vector(\n\u001b[1;32m    404\u001b[0m         embedding,\n\u001b[1;32m    405\u001b[0m         k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_community/vectorstores/faiss.py:154\u001b[0m, in \u001b[0;36mFAISS._embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_function, Embeddings):\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_function(text)\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_google_genai/embeddings.py:252\u001b[0m, in \u001b[0;36mGoogleGenerativeAIEmbeddings.embed_query\u001b[0;34m(self, text, task_type, title, output_dimensionality)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed a text.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m    Embedding for the text.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    251\u001b[0m task_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_type \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETRIEVAL_QUERY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dimensionality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dimensionality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_google_genai/embeddings.py:203\u001b[0m, in \u001b[0;36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[0;34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[0m\n\u001b[1;32m    201\u001b[0m embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    202\u001b[0m batch_start_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mGoogleGenerativeAIEmbeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m titles:\n\u001b[1;32m    205\u001b[0m         titles_batch \u001b[38;5;241m=\u001b[39m titles[\n\u001b[1;32m    206\u001b[0m             batch_start_index : batch_start_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    207\u001b[0m         ]\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_google_genai/embeddings.py:127\u001b[0m, in \u001b[0;36mGoogleGenerativeAIEmbeddings._prepare_batches\u001b[0;34m(texts, batch_size)\u001b[0m\n\u001b[1;32m    120\u001b[0m current_text \u001b[38;5;241m=\u001b[39m texts[text_index]\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Number of tokens per a text is conservatively estimated\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# as 2 times number of words, punctuation and whitespace characters.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Using `count_tokens` API will make batching too expensive.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Utilizing a tokenizer, would add a dependency that would not\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# necessarily be reused by the application using this class.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m current_text_token_cnt \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[43mGoogleGenerativeAIEmbeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_by_punctuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    129\u001b[0m )\n\u001b[1;32m    130\u001b[0m end_of_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_text_token_cnt \u001b[38;5;241m>\u001b[39m _MAX_TOKENS_PER_BATCH:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Current text is too big even for a single batch.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Such request will fail, but we still make a batch\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# so that the app can get the error from the API.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/site-packages/langchain_google_genai/embeddings.py:105\u001b[0m, in \u001b[0;36mGoogleGenerativeAIEmbeddings._split_by_punctuation\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    103\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m([\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_by\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m])\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Using re.split to split the text based on the pattern\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [segment \u001b[38;5;28;01mfor\u001b[39;00m segment \u001b[38;5;129;01min\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m segment]\n",
      "File \u001b[0;32m~/anaconda3/envs/dental_chatbot/lib/python3.9/re.py:231\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(pattern, string, maxsplit, flags)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(pattern, string, maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Split the source string by the occurrences of the pattern,\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m    returning a list containing the resulting substrings.  If\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    capturing parentheses are used in pattern, then the text of all\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    and the remainder of the string is returned as the final element\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m    of the list.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxsplit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "\n",
    "class DentalChatbot:\n",
    "    def __init__(self):\n",
    "        self.llm = LLM()\n",
    "        self.model = self.llm.model\n",
    "        self.retriever = db.as_retriever()\n",
    "        self.chat_history = []\n",
    "        self.feedback_counter = 0\n",
    "        self.translator = Translator()\n",
    "\n",
    "    def get_rag_chain(self):\n",
    "        chain = self.llm.combine_docs_chain()\n",
    "        return create_retrieval_chain(self.retriever, chain)\n",
    "\n",
    "    def answer_question(self, patient_details, query, conversation_history=None, lang='en'):\n",
    "        input_data = {\n",
    "            \"patient_details\": patient_details,\n",
    "            \"query\": query\n",
    "        }\n",
    "\n",
    "        if conversation_history:\n",
    "            history_aware_retriever = create_history_aware_retriever(\n",
    "                self.model, self.retriever, contextualize_q_prompt\n",
    "            )\n",
    "            qa_prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", system_prompt),\n",
    "                    MessagesPlaceholder(\"chat_history\"),\n",
    "                    (\"human\", \"{input}\"),\n",
    "                ]\n",
    "            )\n",
    "            question_answer_chain = create_stuff_documents_chain(self.model, qa_prompt)\n",
    "            rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "            result = rag_chain.invoke({\"input\": input_data, \"chat_history\": conversation_history})\n",
    "            answer = result['answer']\n",
    "        else:\n",
    "            chain = self.get_rag_chain()\n",
    "            result = chain.invoke({\"input\": input_data})\n",
    "            answer = result['answer']\n",
    "\n",
    "        if answer == 'NoIdea':\n",
    "            ai_prompt = PromptTemplate.from_template(user_prompt_model)\n",
    "            chain = ai_prompt | self.llm.model\n",
    "            answer = chain.invoke({\"query\": query})\n",
    "\n",
    "        detected_lang = self.translator.detect(query).lang\n",
    "        if detected_lang != 'en':\n",
    "            answer = self.translator.translate(answer, src='en', dest=detected_lang).text\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def interact(self):\n",
    "        while True:\n",
    "            name = input(\"Enter your name: \")\n",
    "            age = input(\"Enter your age: \")\n",
    "            symptoms = input(\"Describe your symptoms: \")\n",
    "            medical_condition = input(\"Do you have any underlying medical conditions? (yes/no): \")\n",
    "            if medical_condition.lower() == \"yes\":\n",
    "                medical_condition = input(\"Please specify your medical conditions: \")\n",
    "            else:\n",
    "                medical_condition = \"None\"\n",
    "            allergy_history = input(\"Do you have any allergies? (yes/no): \")\n",
    "            if allergy_history.lower() == \"yes\":\n",
    "                allergy_history = input(\"Please specify your allergies: \")\n",
    "            else:\n",
    "                allergy_history = \"None\"\n",
    "            smoker_status = input(\"Are you a smoker? (yes/no): \")\n",
    "            current_dental_history = input(\"When was your last dental check-up? Have you had any major procedures? \")\n",
    "\n",
    "            patient_details = {\n",
    "                \"name\": name,\n",
    "                \"age\": age,\n",
    "                \"symptoms\": symptoms,\n",
    "                \"medical_condition\": medical_condition,\n",
    "                \"allergy_history\": allergy_history,\n",
    "                \"smoker_status\": smoker_status,\n",
    "                \"current_dental_history\": current_dental_history\n",
    "            }\n",
    "\n",
    "            question = input(\"Please ask your dental question: \")\n",
    "            if question.lower() == \"quit\":\n",
    "                break\n",
    "            response = self.answer_question(patient_details=patient_details, query=question, conversation_history=self.chat_history)\n",
    "            print(\"Response:\", response)\n",
    "            self.chat_history.extend([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=response),\n",
    "            ])\n",
    "\n",
    "            self.feedback_counter += 1\n",
    "            if self.feedback_counter % 5 == 0:\n",
    "                user_feedback = input(\"Was this answer helpful? (yes/no): \")\n",
    "                if user_feedback.lower() == \"yes\":\n",
    "                    continue\n",
    "                elif user_feedback.lower() == \"no\":\n",
    "                    new_query = input(\"I'm sorry. Can you please provide more details or ask another question: \")\n",
    "                    if new_query:\n",
    "                        response = self.answer_question(patient_details=patient_details, query=new_query, conversation_history=self.chat_history)\n",
    "                        print(\"Response:\", response)\n",
    "                        self.chat_history.extend([\n",
    "                            HumanMessage(content=new_query),\n",
    "                            AIMessage(content=response),\n",
    "                        ])\n",
    "                    else:\n",
    "                        print(\"Okay, let me know if you have any other questions.\")\n",
    "                        continue\n",
    "\n",
    "# Example\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = DentalChatbot()\n",
    "    chatbot.interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b8ae7d-1081-4172-a0cf-241da7aea9ad",
   "metadata": {},
   "source": [
    "### Step 3: Define function for initializing database with embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8ac695da-9616-41ed-a008-ed00e4df49ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_db_with_embeddings(documents):\n",
    "    print(\"Applying embeddings Google...\")\n",
    "    gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    print(\"Store to FAISS applying gemini embeddings...\")\n",
    "\n",
    "    store = LocalFileStore(\"./cache/\")\n",
    "    cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "        gemini_embeddings, store, namespace=gemini_embeddings.model\n",
    "    )\n",
    "\n",
    "    db = FAISS.from_documents(documents, cached_embedder)\n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e61d863-67a0-4981-aa98-b03bc7c0fc1e",
   "metadata": {},
   "source": [
    "### Step 4: Define function for creating RAG chain with CoT and Google extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df4d64f-fe98-41d4-a18e-a2a80c2baf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_rag_chain(db, prompt):\n",
    "    system_prompt = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        \"{context}\"\n",
    "    )\n",
    "    system_feedback_prompt = (\n",
    "        \"Please provide feedback on the answer:\"\n",
    "        \"\\n\\n\"\n",
    "        \"{context}\"\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ])\n",
    "    feedback_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_feedback_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ])\n",
    "\n",
    "    return create_stuff_documents_chain(\n",
    "        model=VertexAI(model_name=\"gemini-1.0-pro-001\"),\n",
    "        prompt=prompt,\n",
    "        feedback_prompt=feedback_prompt,\n",
    "        retriever=db.as_retriever(),\n",
    "        feedback_handler=collect_feedback,\n",
    "        prompt_context_explainer=None,  # Add a context explainer if needed\n",
    "        use_context_cache=True,\n",
    "        force_reretrieval=False,\n",
    "        retriever_data_override=None,\n",
    "        extract_from_google=True  # Set to True to enable fetching from Google\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee381c-3040-49d6-bf44-af23a62394c1",
   "metadata": {},
   "source": [
    "### Step 5:Define Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4623434b-dcc8-4f23-95ca-e0f061627920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_prompt_template():\n",
    "    system_prompt = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        \"{context}\"\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c3f0c-1aea-49ec-bd75-8619cb87491e",
   "metadata": {},
   "source": [
    "### Step 6: Implement RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c6768452-3b2b-4a9c-bfdf-a38776cee651",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_rag_chain(db, prompt):\n",
    "    model = VertexAI(model_name=\"gemini-1.0-pro-001\")\n",
    "    retriever = db.as_retriever()\n",
    "    question_answer_chain = create_stuff_documents_chain(model, prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "    return rag_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2af756-6064-4542-8917-72f9272281ab",
   "metadata": {},
   "source": [
    "### Step 7: Setup Multilingual Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3439a402-2f21-457a-a137-af729827a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_google_community import GoogleTranslateTransformer\n",
    "\n",
    "def translate_query(query, target_language='en'):\n",
    "    translate_client = GoogleTranslateTransformer(api_key=API_KEY)\n",
    "    translation = translate_client.translate(query, target_language=target_language)\n",
    "    return translation['translatedText']\n",
    "\n",
    "def multilingual_support(query, target_language='en'):\n",
    "    translated_query = translate_query(query, target_language)\n",
    "    return translated_query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc89cfa7-bdb7-4bd9-984d-3794cdf23690",
   "metadata": {},
   "source": [
    "### Step 8: Implement Feedback Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a2f9cc6-2263-4bfc-88bc-c555a172c92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Feedback:\n",
    "    def __init__(self):\n",
    "        self.engine = create_engine('sqlite:///feedback.db', echo=True)\n",
    "        Base = declarative_base()\n",
    "\n",
    "        class Feedback(Base):\n",
    "            __tablename__ = 'feedback'\n",
    "            id = Column(Integer, primary_key=True)\n",
    "            query = Column(String)\n",
    "            rating = Column(Integer)\n",
    "\n",
    "        Base.metadata.create_all(self.engine)\n",
    "        Session = sessionmaker(bind=self.engine)\n",
    "        self.session = Session()\n",
    "\n",
    "    def store_feedback(self, query, rating):\n",
    "        feedback = Feedback(query=query, rating=rating)\n",
    "        self.session.add(feedback)\n",
    "        self.session.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436cb863-d7f5-4d37-9774-f99ed0791f4a",
   "metadata": {},
   "source": [
    "### Step 9: Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd8b3e0-873e-480a-ae64-54e486ec5fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2241e6-8638-4a9c-a272-d6c5af436f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429342e0-dfdf-416d-b3c1-cc6b7ad4bc47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
